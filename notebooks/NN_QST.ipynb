{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f314c5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import scipy.io\n",
    "import matplotlib.pyplot as plt\n",
    "from qiskit.quantum_info import DensityMatrix, random_density_matrix\n",
    "from qiskit.quantum_info.operators import Operator\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3956124f",
   "metadata": {},
   "source": [
    "# First Attempt, Bare Bones Neural Net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee283d9a",
   "metadata": {},
   "source": [
    "# Import the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d8f709f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x0</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>...</th>\n",
       "      <th>y22</th>\n",
       "      <th>y23</th>\n",
       "      <th>y24</th>\n",
       "      <th>y25</th>\n",
       "      <th>y26</th>\n",
       "      <th>y27</th>\n",
       "      <th>y28</th>\n",
       "      <th>y29</th>\n",
       "      <th>y30</th>\n",
       "      <th>y31</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.173828</td>\n",
       "      <td>0.274414</td>\n",
       "      <td>0.490234</td>\n",
       "      <td>0.061523</td>\n",
       "      <td>0.175781</td>\n",
       "      <td>0.259766</td>\n",
       "      <td>0.495117</td>\n",
       "      <td>0.069336</td>\n",
       "      <td>0.318359</td>\n",
       "      <td>0.133789</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011859</td>\n",
       "      <td>0.074632</td>\n",
       "      <td>0.014527</td>\n",
       "      <td>0.011859</td>\n",
       "      <td>-1.032135e-17</td>\n",
       "      <td>-0.156679</td>\n",
       "      <td>-0.083966</td>\n",
       "      <td>-0.074632</td>\n",
       "      <td>0.156679</td>\n",
       "      <td>2.956436e-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.269531</td>\n",
       "      <td>0.179688</td>\n",
       "      <td>0.277344</td>\n",
       "      <td>0.273438</td>\n",
       "      <td>0.293945</td>\n",
       "      <td>0.157227</td>\n",
       "      <td>0.257812</td>\n",
       "      <td>0.291016</td>\n",
       "      <td>0.179688</td>\n",
       "      <td>0.248047</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.031100</td>\n",
       "      <td>0.056268</td>\n",
       "      <td>-0.136595</td>\n",
       "      <td>0.031100</td>\n",
       "      <td>-1.532738e-18</td>\n",
       "      <td>-0.145086</td>\n",
       "      <td>-0.101096</td>\n",
       "      <td>-0.056268</td>\n",
       "      <td>0.145086</td>\n",
       "      <td>1.936221e-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.204102</td>\n",
       "      <td>0.483398</td>\n",
       "      <td>0.210938</td>\n",
       "      <td>0.101562</td>\n",
       "      <td>0.256836</td>\n",
       "      <td>0.434570</td>\n",
       "      <td>0.201172</td>\n",
       "      <td>0.107422</td>\n",
       "      <td>0.196289</td>\n",
       "      <td>0.448242</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.042311</td>\n",
       "      <td>-0.027411</td>\n",
       "      <td>0.095658</td>\n",
       "      <td>0.042311</td>\n",
       "      <td>2.765645e-18</td>\n",
       "      <td>-0.054666</td>\n",
       "      <td>0.046359</td>\n",
       "      <td>0.027411</td>\n",
       "      <td>0.054666</td>\n",
       "      <td>-2.587528e-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.389648</td>\n",
       "      <td>0.251953</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.280273</td>\n",
       "      <td>0.411133</td>\n",
       "      <td>0.244141</td>\n",
       "      <td>0.084961</td>\n",
       "      <td>0.259766</td>\n",
       "      <td>0.488281</td>\n",
       "      <td>0.160156</td>\n",
       "      <td>...</td>\n",
       "      <td>0.104071</td>\n",
       "      <td>0.019399</td>\n",
       "      <td>0.073657</td>\n",
       "      <td>-0.104071</td>\n",
       "      <td>3.922884e-19</td>\n",
       "      <td>0.084765</td>\n",
       "      <td>-0.038264</td>\n",
       "      <td>-0.019399</td>\n",
       "      <td>-0.084765</td>\n",
       "      <td>-5.285627e-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.382812</td>\n",
       "      <td>0.276367</td>\n",
       "      <td>0.140625</td>\n",
       "      <td>0.200195</td>\n",
       "      <td>0.377930</td>\n",
       "      <td>0.229492</td>\n",
       "      <td>0.182617</td>\n",
       "      <td>0.209961</td>\n",
       "      <td>0.426758</td>\n",
       "      <td>0.207031</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.165184</td>\n",
       "      <td>-0.025058</td>\n",
       "      <td>-0.023143</td>\n",
       "      <td>0.165184</td>\n",
       "      <td>5.761045e-18</td>\n",
       "      <td>-0.082392</td>\n",
       "      <td>0.058770</td>\n",
       "      <td>0.025058</td>\n",
       "      <td>0.082392</td>\n",
       "      <td>-3.947200e-18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 68 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         x0        x1        x2        x3        x4        x5        x6  \\\n",
       "0  0.173828  0.274414  0.490234  0.061523  0.175781  0.259766  0.495117   \n",
       "1  0.269531  0.179688  0.277344  0.273438  0.293945  0.157227  0.257812   \n",
       "2  0.204102  0.483398  0.210938  0.101562  0.256836  0.434570  0.201172   \n",
       "3  0.389648  0.251953  0.078125  0.280273  0.411133  0.244141  0.084961   \n",
       "4  0.382812  0.276367  0.140625  0.200195  0.377930  0.229492  0.182617   \n",
       "\n",
       "         x7        x8        x9  ...       y22       y23       y24       y25  \\\n",
       "0  0.069336  0.318359  0.133789  ... -0.011859  0.074632  0.014527  0.011859   \n",
       "1  0.291016  0.179688  0.248047  ... -0.031100  0.056268 -0.136595  0.031100   \n",
       "2  0.107422  0.196289  0.448242  ... -0.042311 -0.027411  0.095658  0.042311   \n",
       "3  0.259766  0.488281  0.160156  ...  0.104071  0.019399  0.073657 -0.104071   \n",
       "4  0.209961  0.426758  0.207031  ... -0.165184 -0.025058 -0.023143  0.165184   \n",
       "\n",
       "            y26       y27       y28       y29       y30           y31  \n",
       "0 -1.032135e-17 -0.156679 -0.083966 -0.074632  0.156679  2.956436e-18  \n",
       "1 -1.532738e-18 -0.145086 -0.101096 -0.056268  0.145086  1.936221e-18  \n",
       "2  2.765645e-18 -0.054666  0.046359  0.027411  0.054666 -2.587528e-18  \n",
       "3  3.922884e-19  0.084765 -0.038264 -0.019399 -0.084765 -5.285627e-18  \n",
       "4  5.761045e-18 -0.082392  0.058770  0.025058  0.082392 -3.947200e-18  \n",
       "\n",
       "[5 rows x 68 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "QST_data = pd.read_csv(\"../data/qst_dataset.csv\")\n",
    "N = QST_data.shape[0]\n",
    "QST_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bc2b6f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clip any small values to zero\n",
    "eps = 1e-10\n",
    "\n",
    "#create boolean mask\n",
    "mask = QST_data.abs() < eps\n",
    "QST_data[mask] =0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ed19bb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cols = []\n",
    "y_cols = []\n",
    "\n",
    "N_x = 36\n",
    "N_y = 32\n",
    "for i in range(N_x):\n",
    "    X_cols.append(f\"x{i}\")\n",
    "for i in range(N_y):\n",
    "    y_cols.append(f\"y{i}\")\n",
    "\n",
    "X = QST_data[X_cols]\n",
    "y = QST_data[y_cols]\n",
    "\n",
    "#split the data\n",
    "X_train, X_test, y_train, y_test=  train_test_split(X, y, train_size=0.7, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "902ead89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x0</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>...</th>\n",
       "      <th>x26</th>\n",
       "      <th>x27</th>\n",
       "      <th>x28</th>\n",
       "      <th>x29</th>\n",
       "      <th>x30</th>\n",
       "      <th>x31</th>\n",
       "      <th>x32</th>\n",
       "      <th>x33</th>\n",
       "      <th>x34</th>\n",
       "      <th>x35</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>836</th>\n",
       "      <td>0.538086</td>\n",
       "      <td>0.216797</td>\n",
       "      <td>0.167969</td>\n",
       "      <td>0.077148</td>\n",
       "      <td>0.497070</td>\n",
       "      <td>0.247070</td>\n",
       "      <td>0.185547</td>\n",
       "      <td>0.070312</td>\n",
       "      <td>0.236328</td>\n",
       "      <td>0.476562</td>\n",
       "      <td>...</td>\n",
       "      <td>0.206055</td>\n",
       "      <td>0.236328</td>\n",
       "      <td>0.496094</td>\n",
       "      <td>0.090820</td>\n",
       "      <td>0.200195</td>\n",
       "      <td>0.212891</td>\n",
       "      <td>0.218750</td>\n",
       "      <td>0.350586</td>\n",
       "      <td>0.168945</td>\n",
       "      <td>0.261719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>575</th>\n",
       "      <td>0.255859</td>\n",
       "      <td>0.140625</td>\n",
       "      <td>0.184570</td>\n",
       "      <td>0.418945</td>\n",
       "      <td>0.270508</td>\n",
       "      <td>0.142578</td>\n",
       "      <td>0.180664</td>\n",
       "      <td>0.406250</td>\n",
       "      <td>0.227539</td>\n",
       "      <td>0.177734</td>\n",
       "      <td>...</td>\n",
       "      <td>0.383789</td>\n",
       "      <td>0.339844</td>\n",
       "      <td>0.087891</td>\n",
       "      <td>0.190430</td>\n",
       "      <td>0.362305</td>\n",
       "      <td>0.359375</td>\n",
       "      <td>0.208008</td>\n",
       "      <td>0.091797</td>\n",
       "      <td>0.301758</td>\n",
       "      <td>0.398438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>0.274414</td>\n",
       "      <td>0.290039</td>\n",
       "      <td>0.215820</td>\n",
       "      <td>0.219727</td>\n",
       "      <td>0.298828</td>\n",
       "      <td>0.320312</td>\n",
       "      <td>0.200195</td>\n",
       "      <td>0.180664</td>\n",
       "      <td>0.087891</td>\n",
       "      <td>0.506836</td>\n",
       "      <td>...</td>\n",
       "      <td>0.362305</td>\n",
       "      <td>0.442383</td>\n",
       "      <td>0.111328</td>\n",
       "      <td>0.056641</td>\n",
       "      <td>0.388672</td>\n",
       "      <td>0.443359</td>\n",
       "      <td>0.040039</td>\n",
       "      <td>0.141602</td>\n",
       "      <td>0.073242</td>\n",
       "      <td>0.745117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1235</th>\n",
       "      <td>0.328125</td>\n",
       "      <td>0.177734</td>\n",
       "      <td>0.158203</td>\n",
       "      <td>0.335938</td>\n",
       "      <td>0.301758</td>\n",
       "      <td>0.163086</td>\n",
       "      <td>0.184570</td>\n",
       "      <td>0.350586</td>\n",
       "      <td>0.317383</td>\n",
       "      <td>0.153320</td>\n",
       "      <td>...</td>\n",
       "      <td>0.285156</td>\n",
       "      <td>0.161133</td>\n",
       "      <td>0.273438</td>\n",
       "      <td>0.278320</td>\n",
       "      <td>0.266602</td>\n",
       "      <td>0.181641</td>\n",
       "      <td>0.403320</td>\n",
       "      <td>0.164062</td>\n",
       "      <td>0.368164</td>\n",
       "      <td>0.064453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1360</th>\n",
       "      <td>0.278320</td>\n",
       "      <td>0.300781</td>\n",
       "      <td>0.204102</td>\n",
       "      <td>0.216797</td>\n",
       "      <td>0.314453</td>\n",
       "      <td>0.280273</td>\n",
       "      <td>0.171875</td>\n",
       "      <td>0.233398</td>\n",
       "      <td>0.133789</td>\n",
       "      <td>0.475586</td>\n",
       "      <td>...</td>\n",
       "      <td>0.268555</td>\n",
       "      <td>0.177734</td>\n",
       "      <td>0.199219</td>\n",
       "      <td>0.348633</td>\n",
       "      <td>0.272461</td>\n",
       "      <td>0.179688</td>\n",
       "      <td>0.231445</td>\n",
       "      <td>0.345703</td>\n",
       "      <td>0.174805</td>\n",
       "      <td>0.248047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1130</th>\n",
       "      <td>0.290039</td>\n",
       "      <td>0.147461</td>\n",
       "      <td>0.307617</td>\n",
       "      <td>0.254883</td>\n",
       "      <td>0.306641</td>\n",
       "      <td>0.153320</td>\n",
       "      <td>0.307617</td>\n",
       "      <td>0.232422</td>\n",
       "      <td>0.333008</td>\n",
       "      <td>0.146484</td>\n",
       "      <td>...</td>\n",
       "      <td>0.344727</td>\n",
       "      <td>0.094727</td>\n",
       "      <td>0.274414</td>\n",
       "      <td>0.255859</td>\n",
       "      <td>0.348633</td>\n",
       "      <td>0.121094</td>\n",
       "      <td>0.169922</td>\n",
       "      <td>0.392578</td>\n",
       "      <td>0.308594</td>\n",
       "      <td>0.128906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1294</th>\n",
       "      <td>0.151367</td>\n",
       "      <td>0.365234</td>\n",
       "      <td>0.085938</td>\n",
       "      <td>0.397461</td>\n",
       "      <td>0.113281</td>\n",
       "      <td>0.409180</td>\n",
       "      <td>0.090820</td>\n",
       "      <td>0.386719</td>\n",
       "      <td>0.339844</td>\n",
       "      <td>0.180664</td>\n",
       "      <td>...</td>\n",
       "      <td>0.058594</td>\n",
       "      <td>0.197266</td>\n",
       "      <td>0.143555</td>\n",
       "      <td>0.612305</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.181641</td>\n",
       "      <td>0.483398</td>\n",
       "      <td>0.251953</td>\n",
       "      <td>0.112305</td>\n",
       "      <td>0.152344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>0.206055</td>\n",
       "      <td>0.248047</td>\n",
       "      <td>0.131836</td>\n",
       "      <td>0.414062</td>\n",
       "      <td>0.206055</td>\n",
       "      <td>0.212891</td>\n",
       "      <td>0.144531</td>\n",
       "      <td>0.436523</td>\n",
       "      <td>0.286133</td>\n",
       "      <td>0.137695</td>\n",
       "      <td>...</td>\n",
       "      <td>0.191406</td>\n",
       "      <td>0.479492</td>\n",
       "      <td>0.127930</td>\n",
       "      <td>0.202148</td>\n",
       "      <td>0.215820</td>\n",
       "      <td>0.454102</td>\n",
       "      <td>0.142578</td>\n",
       "      <td>0.186523</td>\n",
       "      <td>0.325195</td>\n",
       "      <td>0.345703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1459</th>\n",
       "      <td>0.221680</td>\n",
       "      <td>0.118164</td>\n",
       "      <td>0.233398</td>\n",
       "      <td>0.426758</td>\n",
       "      <td>0.214844</td>\n",
       "      <td>0.112305</td>\n",
       "      <td>0.228516</td>\n",
       "      <td>0.444336</td>\n",
       "      <td>0.268555</td>\n",
       "      <td>0.047852</td>\n",
       "      <td>...</td>\n",
       "      <td>0.344727</td>\n",
       "      <td>0.215820</td>\n",
       "      <td>0.091797</td>\n",
       "      <td>0.320312</td>\n",
       "      <td>0.364258</td>\n",
       "      <td>0.223633</td>\n",
       "      <td>0.258789</td>\n",
       "      <td>0.169922</td>\n",
       "      <td>0.293945</td>\n",
       "      <td>0.277344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1126</th>\n",
       "      <td>0.296875</td>\n",
       "      <td>0.196289</td>\n",
       "      <td>0.131836</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.317383</td>\n",
       "      <td>0.198242</td>\n",
       "      <td>0.133789</td>\n",
       "      <td>0.350586</td>\n",
       "      <td>0.177734</td>\n",
       "      <td>0.334961</td>\n",
       "      <td>...</td>\n",
       "      <td>0.274414</td>\n",
       "      <td>0.282227</td>\n",
       "      <td>0.174805</td>\n",
       "      <td>0.264648</td>\n",
       "      <td>0.286133</td>\n",
       "      <td>0.274414</td>\n",
       "      <td>0.123047</td>\n",
       "      <td>0.293945</td>\n",
       "      <td>0.426758</td>\n",
       "      <td>0.156250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1400 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            x0        x1        x2        x3        x4        x5        x6  \\\n",
       "836   0.538086  0.216797  0.167969  0.077148  0.497070  0.247070  0.185547   \n",
       "575   0.255859  0.140625  0.184570  0.418945  0.270508  0.142578  0.180664   \n",
       "557   0.274414  0.290039  0.215820  0.219727  0.298828  0.320312  0.200195   \n",
       "1235  0.328125  0.177734  0.158203  0.335938  0.301758  0.163086  0.184570   \n",
       "1360  0.278320  0.300781  0.204102  0.216797  0.314453  0.280273  0.171875   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1130  0.290039  0.147461  0.307617  0.254883  0.306641  0.153320  0.307617   \n",
       "1294  0.151367  0.365234  0.085938  0.397461  0.113281  0.409180  0.090820   \n",
       "860   0.206055  0.248047  0.131836  0.414062  0.206055  0.212891  0.144531   \n",
       "1459  0.221680  0.118164  0.233398  0.426758  0.214844  0.112305  0.228516   \n",
       "1126  0.296875  0.196289  0.131836  0.375000  0.317383  0.198242  0.133789   \n",
       "\n",
       "            x7        x8        x9  ...       x26       x27       x28  \\\n",
       "836   0.070312  0.236328  0.476562  ...  0.206055  0.236328  0.496094   \n",
       "575   0.406250  0.227539  0.177734  ...  0.383789  0.339844  0.087891   \n",
       "557   0.180664  0.087891  0.506836  ...  0.362305  0.442383  0.111328   \n",
       "1235  0.350586  0.317383  0.153320  ...  0.285156  0.161133  0.273438   \n",
       "1360  0.233398  0.133789  0.475586  ...  0.268555  0.177734  0.199219   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "1130  0.232422  0.333008  0.146484  ...  0.344727  0.094727  0.274414   \n",
       "1294  0.386719  0.339844  0.180664  ...  0.058594  0.197266  0.143555   \n",
       "860   0.436523  0.286133  0.137695  ...  0.191406  0.479492  0.127930   \n",
       "1459  0.444336  0.268555  0.047852  ...  0.344727  0.215820  0.091797   \n",
       "1126  0.350586  0.177734  0.334961  ...  0.274414  0.282227  0.174805   \n",
       "\n",
       "           x29       x30       x31       x32       x33       x34       x35  \n",
       "836   0.090820  0.200195  0.212891  0.218750  0.350586  0.168945  0.261719  \n",
       "575   0.190430  0.362305  0.359375  0.208008  0.091797  0.301758  0.398438  \n",
       "557   0.056641  0.388672  0.443359  0.040039  0.141602  0.073242  0.745117  \n",
       "1235  0.278320  0.266602  0.181641  0.403320  0.164062  0.368164  0.064453  \n",
       "1360  0.348633  0.272461  0.179688  0.231445  0.345703  0.174805  0.248047  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "1130  0.255859  0.348633  0.121094  0.169922  0.392578  0.308594  0.128906  \n",
       "1294  0.612305  0.062500  0.181641  0.483398  0.251953  0.112305  0.152344  \n",
       "860   0.202148  0.215820  0.454102  0.142578  0.186523  0.325195  0.345703  \n",
       "1459  0.320312  0.364258  0.223633  0.258789  0.169922  0.293945  0.277344  \n",
       "1126  0.264648  0.286133  0.274414  0.123047  0.293945  0.426758  0.156250  \n",
       "\n",
       "[1400 rows x 36 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da489dc",
   "metadata": {},
   "source": [
    "# Training the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c0c264d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "QST_NN = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer(shape=(N_x,)),\n",
    "    tf.keras.layers.Dense(50, activation=\"relu\"),       # 2 dense layers, TOBE modified\n",
    "    tf.keras.layers.Dense(50, activation=\"relu\"),       # 2 dense layers, TOBE modified\n",
    "    tf.keras.layers.Dense(N_y)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ed4461c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "QST_NN.compile(\n",
    "    optimizer = 'adam',\n",
    "    loss = 'mse',\n",
    "    metrics=['mse']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "55fb2f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0200 - mse: 0.0200 - val_loss: 0.0074 - val_mse: 0.0074\n",
      "Epoch 2/20\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 818us/step - loss: 0.0070 - mse: 0.0070 - val_loss: 0.0060 - val_mse: 0.0060\n",
      "Epoch 3/20\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 761us/step - loss: 0.0059 - mse: 0.0059 - val_loss: 0.0052 - val_mse: 0.0052\n",
      "Epoch 4/20\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 793us/step - loss: 0.0051 - mse: 0.0051 - val_loss: 0.0045 - val_mse: 0.0045\n",
      "Epoch 5/20\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 805us/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0040 - val_mse: 0.0040\n",
      "Epoch 6/20\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 806us/step - loss: 0.0040 - mse: 0.0040 - val_loss: 0.0037 - val_mse: 0.0037\n",
      "Epoch 7/20\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 755us/step - loss: 0.0037 - mse: 0.0037 - val_loss: 0.0035 - val_mse: 0.0035\n",
      "Epoch 8/20\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 802us/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0034 - val_mse: 0.0034\n",
      "Epoch 9/20\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 794us/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0034 - val_mse: 0.0034\n",
      "Epoch 10/20\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 763us/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0034 - val_mse: 0.0034\n",
      "Epoch 11/20\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 787us/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0033 - val_mse: 0.0033\n",
      "Epoch 12/20\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0033 - mse: 0.0033 - val_loss: 0.0033 - val_mse: 0.0033\n",
      "Epoch 13/20\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 831us/step - loss: 0.0033 - mse: 0.0033 - val_loss: 0.0033 - val_mse: 0.0033\n",
      "Epoch 14/20\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 964us/step - loss: 0.0033 - mse: 0.0033 - val_loss: 0.0033 - val_mse: 0.0033\n",
      "Epoch 15/20\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 796us/step - loss: 0.0033 - mse: 0.0033 - val_loss: 0.0033 - val_mse: 0.0033\n",
      "Epoch 16/20\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 786us/step - loss: 0.0033 - mse: 0.0033 - val_loss: 0.0033 - val_mse: 0.0033\n",
      "Epoch 17/20\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 782us/step - loss: 0.0033 - mse: 0.0033 - val_loss: 0.0033 - val_mse: 0.0033\n",
      "Epoch 18/20\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 797us/step - loss: 0.0032 - mse: 0.0032 - val_loss: 0.0032 - val_mse: 0.0032\n",
      "Epoch 19/20\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 779us/step - loss: 0.0033 - mse: 0.0033 - val_loss: 0.0033 - val_mse: 0.0033\n",
      "Epoch 20/20\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 746us/step - loss: 0.0033 - mse: 0.0033 - val_loss: 0.0033 - val_mse: 0.0033\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x316423860>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QST_NN.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    validation_split=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "38cd3d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 319us/step - loss: 0.0032 - mse: 0.0032\n",
      "test loss: 0.003218843601644039\n"
     ]
    }
   ],
   "source": [
    "y_pred = QST_NN.predict(X_test)\n",
    "\n",
    "test_loss, _ = QST_NN.evaluate(X_test, y_test)\n",
    "\n",
    "print(f\"test loss: {test_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ef941e",
   "metadata": {},
   "source": [
    "## Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e40b2a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.003218843504856142\n",
      "R^2: : 0.43025660514831543\n"
     ]
    }
   ],
   "source": [
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R^2: : {r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8fec46",
   "metadata": {},
   "source": [
    "# Second Attempt: Using more Advanced Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3674a7c",
   "metadata": {},
   "source": [
    "## 1) Data PreProccessing\n",
    "    \n",
    "- Cholesky Decomposition on targets rho\n",
    "- Principal component analysis on data X (frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6754ba46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6588d690",
   "metadata": {},
   "source": [
    "## 2) Building Advanced Neural Network \n",
    "- Batch Normalisation\n",
    "- Enforce positivity and Trace 1 on outputs\n",
    "- Regularisation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
